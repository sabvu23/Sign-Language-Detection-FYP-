from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

os.environ["CUDA_VISIBLE_DEVICES"] = "1"
sz = 128

# Step 1 - Building the CNN
classifier = Sequential()

# First convolution layer and pooling
classifier.add(Conv2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))
classifier.add(MaxPooling2D(pool_size=(2, 2)))

# Second convolution layer and pooling
classifier.add(Conv2D(32, (3, 3), activation='relu'))
classifier.add(MaxPooling2D(pool_size=(2, 2)))

# Flattening the layers
classifier.add(Flatten())

# Fully connected layers
classifier.add(Dense(units=128, activation='relu'))
classifier.add(Dropout(0.40))
classifier.add(Dense(units=96, activation='relu'))
classifier.add(Dropout(0.40))
classifier.add(Dense(units=64, activation='relu'))
classifier.add(Dense(units=29, activation='softmax'))  # Adjust to match detected classes

# Compile the CNN
classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 2 - Preparing Data
train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)

train_dir = 'C:/Users/Asus/Desktop/ASLDataset/Dataset/asl_alphabet_train_split/train'
test_dir = 'C:/Users/Asus/Desktop/ASLDataset/Dataset/asl_alphabet_train_split/test'

# Load datasets
training_set = train_datagen.flow_from_directory(
    train_dir,
    target_size=(sz, sz),
    batch_size=10,
    color_mode='grayscale',
    class_mode='categorical'
)
test_set = test_datagen.flow_from_directory(
    test_dir,
    target_size=(sz, sz),
    batch_size=10,
    color_mode='grayscale',
    class_mode='categorical'
)

# Verify number of classes
print(f"Training classes: {len(training_set.class_indices)}")
print(f"Test classes: {len(test_set.class_indices)}")

# Training
classifier.fit(
    training_set,
    steps_per_epoch=training_set.samples // training_set.batch_size,
    epochs=10,
    validation_data=test_set,
    validation_steps=test_set.samples // test_set.batch_size
)

# Evaluate
test_loss, test_accuracy = classifier.evaluate(test_set)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

# Save Model
classifier.save('model-bw.h5')
print('Model saved successfully!')
